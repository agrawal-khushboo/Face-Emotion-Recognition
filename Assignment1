from os import listdir
from PIL import Image
import numpy as np
from numpy import linalg as LA
import math

# The relative path to your CAFE-Gamma dataset
data_dir = "./CAFE/"

# Dictionary of semantic "label" to emotions
emotion_dict = {"h": "happy", "ht": "happy with teeth", "m": "maudlin",
                "s": "surprise", "f": "fear", "a": "anger", "d": "disgust", "n": "neutral"}
Happydata = []
saddata = []
images = []
labels = []

def load_data(data_dir):
    """ Load all PGM images stored in your data directory into a list of NumPy
    arrays with a list of corresponding labels.

    Args:
        data_dir: The relative filepath to the CAFE dataset.
    Returns:
        images: A list containing every image in CAFE as an array.
        labels: A list of the corresponding labels (filenames) for each image.
    """
    # Get the list of image file names
    all_files = listdir(data_dir)

    # Store the images as arrays and their labels in two lists


    for file in all_files:

        # Load in the files as PIL images and convert to NumPy arrays
        if '_ht' in file:
            img = Image.open(data_dir + file)
            Happydata.append(np.array(img))
            images.append(np.array(img))
            labels.append(file)
        elif '_m' in file:
            img = Image.open(data_dir + file)
            saddata.append(np.array(img))
            images.append(np.array(img))
            labels.append(file)

        elif '_h' in file or '_n' in file:
            continue
        else:
            img = Image.open(data_dir + file)
            images.append(np.array(img))
            labels.append(file)

    print("Total number of images:", len(images), "and labels:", len(labels))

    return images, labels


def PCA(data, dims_rescaled_data):
    """
    returns: data transformed in 2 dims/columns + regenerated original data
    pass in: data as 2D NumPy array
    """
    m, n = data.shape
    # mean center the data
    data = data - np.mean(data, axis=0)
    # calculate the covariance matrix
    R = np.matmul(data, data.T)
    # calculate eigenvectors & eigenvalues of the covariance matrix
    # use 'eigh' rather than 'eig' since R is symmetric,
    # the . gain is substantial
    evals, evecs = LA.eigh(R)
    # sort eigenvalue in decreasing order
    idx = np.argsort(evals)[::-1]
    evecs = evecs[:, idx]
    # sort eigenvectors according to same index
    evals = evals[idx]
    # select the first n eigenvectors (n is desired dimension
    # of rescaled data array, or dims_rescaled_data)
    evecs = evecs[:, :dims_rescaled_data]
    # carry out the transformation on the data using eigenvectors
    # and return the re-scaled data, eigenvalues, and eigenvectors
    eigen_vectors = np.dot(data.T, evecs)
    for i in range(eigen_vectors.shape[1]):
        sum = np.linalg.norm(eigen_vectors[:, i])
        eigen_vectors[:, i] = eigen_vectors[:, i] / sum
        eigen_vectors[:, i] = eigen_vectors[:, i] / math.sqrt(evals[i])

    return data, evals, eigen_vectors


def display_face(img):
    """ Display the input image and optionally save as a PNG.

    Args:
        img: The NumPy array or image to display

    Returns: None
    """
    # Convert img to PIL Image object (if it's an ndarray)
    if type(img) == np.ndarray:
        print("Converting from array to PIL Image")
        img = Image.fromarray(img)

    # Display the image
    img.show()


def reduce_dimensions(Data):
    Data_new = []
    for image in Data:
        a = image.flatten()
        Data_new.append(a)

    Data_new = np.matrix(Data_new)
    _, ev, ei = PCA(Data_new, len(Data) - 1)
    reduced_data = np.dot(Data_new, ei)
    return reduced_data, ei


images, labels = load_data(data_dir="./CAFE/")
Data, ei = reduce_dimensions(images)


def sigmoid(value):
    return 1.0 / (1 + np.exp(-value))


def calculate_accuracy(test,testlabel,weights):
    correct = 0
    for i in range(test.shape[0]):
        value = np.dot(weights, test[i].T)
        predict = sigmoid(value)
        if predict >=0.5:
            y =1
        else:
            y=0
        if y == testlabel[i]:
            correct +=1
    return correct/test.shape[0]


def lossfunction(validation, validationlabel, weights):
    loss = 0
    for i in range(validation.shape[0]):
        value = np.dot(weights, validation[i].T)
        predict = sigmoid(value)
        loss += validationlabel[i] * np.log(predict) + (1 - validationlabel[i]) * np.log(1 - predict)
    loss = loss*1.0/validation.shape[0]
    return -loss


def logistic_regression(features, target, epochs, rate, validation, validationlabel):
    weights = np.zeros(features.shape[1])
    updated_weights = np.zeros(features.shape[1])
    loss = lossfunction(validation,validationlabel,weights)
    for i in range(epochs):
        gradient = 0
        for j in range(0, features.shape[0]):
            value = np.dot(weights,features[j].T)
            predict = sigmoid(value)
            gradient += (target[j] - predict) * features[j]
        updated_weights = weights + rate*gradient
        new_loss = lossfunction(validation,validationlabel,updated_weights)
        if new_loss < loss:
            loss = new_loss
            weights = updated_weights
    return weights
avg_error = 0
avg_accuracy = 0

#logistic regression happy v/s sad
for i in range(10):
    train = []
    validation = []
    trainlabel = []
    testlabel = []
    newvalidation = []
    validationlabel = []
    test = []
    if i < 9:
        for k in range(10):
            if k == i:
                test.append(Happydata[k])
                testlabel.append(1)
                test.append(saddata[k])
                testlabel.append(0)
            elif k == i + 1:
                validation.append(Happydata[k])
                validationlabel.append(1)
                validation.append(saddata[k])
                validationlabel.append(0)
            else:
                train.append(Happydata[k])
                trainlabel.append(1)
                train.append(saddata[k])
                trainlabel.append(0)
    else:
        test.append(Happydata[k])
        testlabel.append(1)
        test.append(saddata[k])
        testlabel.append(0)
        validation.append(Happydata[0])
        validationlabel.append(1)
        validation.append(saddata[0])
        validationlabel.append(0)
        for k in range(1, 9):
            train.append(Happydata[k])
            trainlabel.append(1)
            train.append(saddata[k])
            trainlabel.append(0)
    train, eigenvec = reduce_dimensions(train)
    for image in validation:
        newvalidation.append(image.flatten())

    newvalidation = np.matrix(newvalidation)

    newvalidation = np.matmul(newvalidation, eigenvec)
    w = logistic_regression(train, trainlabel, 10, 0.1, newvalidation, validationlabel)
    newtest = []
    for image in test:
        newtest.append(image.flatten())

    newtest = np.matrix(newtest)

    newtest = np.matmul(newtest, eigenvec)
    error = lossfunction(newtest,testlabel,w)
    avg_error += error
    accuracy = calculate_accuracy(newtest,testlabel,w)
    avg_accuracy += accuracy

avg_error = avg_error*1.0/10
avg_accuracy = avg_accuracy*1.0/10



#softmax regression for all six emaotions

def yn(weights,features):
    s=0
    y=[]
    value=np.matmul(features,weights)
    value=np.log(value)
    value=np.array(value)
    for i in range(len(value)):
        s=np.sum(value[i])
        y.append(value[i]/s)
    y=np.matrix(y)
    return y

def softmax_loss(validation,validationlabel,weights):
    loss=0
    y=yn(weights,validation)
    y=np.array(y)
    for i in range(validation.shape[0]):
        for j in range(6):
            loss+= validationlabel[i][j]*np.log(y[i][j])
    return -loss

def softmax_regression(features, target, epochs, rate, validation, validationlabel):
    weights=np.zeros((features.shape[1],6), dtype=np.int)
    new_weights=np.zeros((features.shape[1],6), dtype=np.int)
    loss= softmax_loss(validation,validationlabel,weights)
    for i in range(epochs):
        gradient=0
        predict=yn(weights,features)
        predict=np.array(predict)
        for j in range(features.shape[0]):
            gradient+= np.matmul(features[j].T,np.matrix(target[j]-predict[j]))
        new_weights=weights+rate*gradient
        new_loss=softmax_loss(validation,validationlabel,new_weights)
        if new_loss<loss:
            loss=new_loss
            weights=new_weights
    return weights


for i in range(10):
    train = []
    validation = []
    trainlabel = []
    testlabel = []
    newvalidation = []
    validationlabel = []
    test = []
    if i < 9:
        for k in range(60):
            if '_h' in labels[k]:
                label = [1, 0, 0, 0, 0, 0]
            elif '_m' in labels[k]:
                label = [0, 1, 0, 0, 0, 0]
            elif '_s' in labels[k]:
                label = [0, 0, 1, 0, 0, 0]
            elif '_f' in labels[k]:
                label = [0, 0, 0, 1, 0, 0]
            elif '_a' in labels[k]:
                label = [0, 0, 0, 0, 1, 0]
            else:
                label = [0, 0, 0, 0, 0, 1]
            if k >= 6*i and k < 6*(i+1):
                test.append(images[k])
                testlabel.append(label)
            elif k >= 6*(i+1) and k < 6*(i+2):
                validation.append(images[k])
                validationlabel.append(label)
            else:
                train.append(images[k])
                trainlabel.append(label)
    else:
        for k in range(6*i,6*(i+1)):
            if '_h' in labels[k]:
                label = [1, 0, 0, 0, 0, 0]
            elif '_m' in labels[k]:
                label = [0, 1, 0, 0, 0, 0]
            elif '_s' in labels[k]:
                label = [0, 0, 1, 0, 0, 0]
            elif '_f' in labels[k]:
                label = [0, 0, 0, 1, 0, 0]
            elif '_a' in labels[k]:
                label = [0, 0, 0, 0, 1, 0]
            else:
                label = [0, 0, 0, 0, 0, 1]
            test.append(images[k])
            testlabel.append(label)
        for k in range(0,6):
            if '_h' in labels[k]:
                label = [1, 0, 0, 0, 0, 0]
            elif '_m' in labels[k]:
                label = [0, 1, 0, 0, 0, 0]
            elif '_s' in labels[k]:
                label = [0, 0, 1, 0, 0, 0]
            elif '_f' in labels[k]:
                label = [0, 0, 0, 1, 0, 0]
            elif '_a' in labels[k]:
                label = [0, 0, 0, 0, 1, 0]
            else:
                label = [0, 0, 0, 0, 0, 1]
            validation.append(images[k])
            validationlabel.append(label)
        for k in range(6,54):
            if '_h' in labels[k]:
                label = [1, 0, 0, 0, 0, 0]
            elif '_m' in labels[k]:
                label = [0, 1, 0, 0, 0, 0]
            elif '_s' in labels[k]:
                label = [0, 0, 1, 0, 0, 0]
            elif '_f' in labels[k]:
                label = [0, 0, 0, 1, 0, 0]
            elif '_a' in labels[k]:
                label = [0, 0, 0, 0, 1, 0]
            else:
                label = [0, 0, 0, 0, 0, 1]
            train.append(images[k])
            trainlabel.append(label)
            
    train, eigenvec = reduce_dimensions(train)
    trainlabel=np.matrix(trainlabel)
#     validationlabel=np.matrix(validationlabel)
#     testlabel=np.matrix(testlabel)
    for image in validation:
        newvalidation.append(image.flatten())

    newvalidation = np.matrix(newvalidation)

    newvalidation = np.matmul(newvalidation, eigenvec)
    newtest = []
    for image in test:
        newtest.append(image.flatten())

    newtest = np.matrix(newtest)

    newtest = np.matmul(newtest, eigenvec)
    print(newtest.shape)
    w = softmax_regression(train, trainlabel, 10, 0.1, newvalidation, validationlabel)
    print(w)
    
